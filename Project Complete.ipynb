{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import spacy \n",
    "import sqlite3\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import tree\n",
    "import lime \n",
    "import lime.lime_tabular\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 60)\n",
    "from IPython.core.pylabtools import figsize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "le = LabelEncoder()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#change sample size\n",
    "sample_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the amazon reviews CSV file\n",
    "#Df is the database we will be using for our analysis\n",
    "#fulldf was created to be able to tests assumptions \n",
    "df = pd.read_csv(\"Reviews.csv\")\n",
    "fulldf = pd.read_csv(\"Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting infomration on dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be able to proprely run the program we took a fraction of the dataset \n",
    "#random subset of the total dataset\n",
    "#frac = 0.05 is 5% of the dataset\n",
    "df = df.sample(frac = 0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this gives information on the new dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 5 rows of the sample dataset\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first step of clean is taking the needed columns \n",
    "df = df[[\"Score\",\"ProductId\",\"Summary\",\"Text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function cleans up all special characters generated in the amazon review database \n",
    "def clean(s):\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'\\d+','', s)\n",
    "    s = s.replace(r'<lb>', \"\\n\")\n",
    "    s = re.sub(r'<br */*>', \"\\n\", s)\n",
    "    s = s.replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\n",
    "    s = s.replace(\"&amp;\", \"&\")\n",
    "    s = re.sub(r'\\(*https*://[^\\)]*\\)*', \"\", s)\n",
    "    s = re.sub(r'\\*', '', s)\n",
    "    s = re.sub(r'_+', ' ', s)\n",
    "    s = re.sub(r'\"+', '\"', s)\n",
    "    s = re.sub(r\"[-()\\\"#/&@;:<>{}`+=~|.!?,]\", \"\", s)\n",
    "    s = re.sub(r'[^\\w]', ' ', s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", s)\n",
    "    s = re.sub(r\"what's\", \"what is \", s)\n",
    "    s = re.sub(r\"\\'s\", \" \", s)\n",
    "    s = re.sub(r\"\\'ve\", \" have \", s)\n",
    "    s = re.sub(r\"n't\", \" not \", s)\n",
    "    s = re.sub(r\"i'm\", \"i am \", s)\n",
    "    s = re.sub(r\"\\'re\", \" are \", s)\n",
    "    s = re.sub(r\"\\'d\", \" would \", s)\n",
    "    s = re.sub(r\"\\'ll\", \" will \", s)\n",
    "    s = re.sub(r\",\", \" \", s)\n",
    "    s = re.sub(r\"\\.\", \" \", s)\n",
    "    s = re.sub(r\"!\", \" ! \", s)\n",
    "    s = re.sub(r\"\\/\", \" \", s)\n",
    "    s = re.sub(r\"\\^\", \" ^ \", s)\n",
    "    s = re.sub(r\"\\+\", \" + \", s)\n",
    "    s = re.sub(r\"\\-\", \" - \", s)\n",
    "    s = re.sub(r\"\\=\", \" = \", s)\n",
    "    s = re.sub(r\"'\", \" \", s)\n",
    "    s = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", s)\n",
    "    s = re.sub(r\":\", \" : \", s)\n",
    "    s = re.sub(r\" e g \", \" eg \", s)\n",
    "    s = re.sub(r\" b g \", \" bg \", s)\n",
    "    s = re.sub(r\"\\0s\", \"0\", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    \n",
    "    s = s.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in s]\n",
    "    s = \" \".join(stemmed_words)\n",
    "\n",
    "    return(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 5 rows of the dataset\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph plotting the distribution of scores and sentiment of the customers \n",
    "#dataset is unbalanced\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "df.groupby(\"Score\").Summary.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency of unbalanced dataset \n",
    "df[[\"Score\",\"ProductId\",\"Summary\",\"Text\"]].describe(include=np.object).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we have an unbalanced dataset we reloaded the full dataset\n",
    "#to be able to balance it \n",
    "shuffled_df = fulldf.sample(frac=1,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df.info()\n",
    "shuffled_df[\"Score\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each of these represents all the reviews with a score from 1 to 5 which is stored into a seperate sets \n",
    "one_df = shuffled_df.loc[shuffled_df['Score'] == 1]\n",
    "two_df = shuffled_df.loc[shuffled_df['Score'] == 2]\n",
    "four_df = shuffled_df.loc[shuffled_df['Score'] == 4]\n",
    "five_df = shuffled_df.loc[shuffled_df['Score'] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews with score of two had 29k reviews we used an undersampling method to set all the 5 categories to 25k\n",
    "one_df = shuffled_df.loc[shuffled_df['Score'] == 1].sample(n=25000,random_state=42)\n",
    "two_df = shuffled_df.loc[shuffled_df['Score'] == 2].sample(n=25000,random_state=42)\n",
    "four_df = shuffled_df.loc[shuffled_df['Score'] == 4].sample(n=25000,random_state=42)\n",
    "five_df = shuffled_df.loc[shuffled_df['Score'] == 5].sample(n=25000,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentiment(Score):\n",
    "    if Score == 1 or Score == 2:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"positive\"\n",
    "    \n",
    "    \n",
    "def binary_sentiment(sentiment):\n",
    "    if sentiment == \"negative\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined all 5 datasets into one to create our new balanced dataset \n",
    "normalized_df = pd.concat([one_df, two_df, four_df, five_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df['sentiment'] = normalized_df[\"Score\"].apply (score_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df[\"binary_sentiment\"] = normalized_df[\"sentiment\"].apply(binary_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot shows that each of the scores has 25k each for a total of 125k entries \n",
    "fig = plt.figure(figsize=(8,6))\n",
    "normalized_df.groupby(\"Score\").Summary.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#information on th dataset\n",
    "normalized_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we reduced our dataset to 15% of the 125k to be able to run the data properly \n",
    "#took a random sampling the the dataset \n",
    "df = normalized_df.sample(n = sample_size,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph reprsenting each of the scores number of datapoints \n",
    "fig = plt.figure(figsize=(8,6))\n",
    "df.groupby(\"Score\").Summary.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "#only kept the columns we needed\n",
    "#used the clean function to remove special characters\n",
    "\n",
    "df = df[['ProductId','Score','Summary','Text', 'sentiment', \"binary_sentiment\"]]  # Using only the useful columns\n",
    "df[\"Text\"] = df.Text.apply(clean)\n",
    "df[\"Summary\"] = df.Summary.apply(clean)\n",
    "#clean the dataset further by removing punctuation\n",
    "import string\n",
    "table = str.maketrans(' ', ' ', string.punctuation)\n",
    "df[\"Text\"] = [w.translate(table) for w in df[\"Text\"]]\n",
    "df[\"Summary\"] = [w.translate(table) for w in df[\"Summary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)  # Outputting the data set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removed all duplicate reviews\n",
    "df[[\"Score\",\"ProductId\",\"Summary\",\"Text\", 'sentiment']].describe(include=np.object).transpose()  #We noticed that there is 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset =\"Text\",keep = False, inplace = True) # So we deleted duplicate reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()                            #info of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Score\",\"ProductId\",\"Summary\",\"Text\", 'sentiment']].describe(include=np.object).transpose() #We no longer have duplicate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()  # Average rating of products "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words function\n",
    "#added t.lower because the stop_words are all lower case \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#function to remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords from a list of tokens.\"\"\"\n",
    "    return [t for t in tokens if t.lower() not in stop_words]\n",
    "\n",
    "#function takes most common words and puts them in a list \n",
    "#the function will pick up the number of times a words comes in a dictiiiiiiiionnary\n",
    "#takes the word as a key and number as a value\n",
    "def return_uncommon(words, number):\n",
    "    tuple_len = len(words)\n",
    "    i = 0\n",
    "    cleaned_list = []\n",
    "    \n",
    "    while i!=tuple_len:\n",
    "        \n",
    "         for key, repetition in words.items():\n",
    "           \n",
    "             number_repetition = repetition\n",
    "             if number_repetition <= number:\n",
    "                cleaned_words = key\n",
    "                cleaned_list.append(cleaned_words)\n",
    "             i+=1\n",
    "    return cleaned_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ran our tfidfvectoriser with the summary\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=10, norm='l2', encoding='latin-1', \n",
    "                        ngram_range=(1, 2), stop_words=stop_words)\n",
    "\n",
    "features = tfidf.fit_transform(df.Text).toarray()\n",
    "\n",
    "labels = df.sentiment\n",
    "labels_binary = df.binary_sentiment\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created a function to split sentences into words vectors (tokens)\n",
    "\n",
    "def my_tokenizer(text):\n",
    "    return text.split() if text != (None or \".\" or \",\" or \"!\" or \")\" or \"(\" or \"\\\"\") else []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df.Text.map(my_tokenizer).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of words before removal of stopwords\n",
    "#we notice that most words are stopwords\n",
    "from collections import Counter\n",
    "#count the number of times the tokens come\n",
    "counter = Counter(tokens) \n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#count words without stopwords \n",
    "counter = Counter(remove_stopwords(tokens))\n",
    "list_counter = dict(counter)\n",
    "uncommon_words = return_uncommon(list_counter,2)\n",
    "stop_words = stop_words.union(uncommon_words)\n",
    "counter = Counter(remove_stopwords(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20 most common words \n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency plot of the most common words \n",
    "freq_df = pd.DataFrame.from_records(counter.most_common(20), columns = [\"token\", \"count\"])\n",
    "\n",
    "freq_df.plot(kind=\"bar\", x=\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting the 20 most common bigrams \n",
    "bi_grams = list(ngrams(remove_stopwords(tokens), 2)) \n",
    "bi_counter = Counter(bi_grams)\n",
    "bi_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency plot of the most common bigrams \n",
    "freq_df = pd.DataFrame.from_records(bi_counter.most_common(20), columns = [\"token\", \"count\"])\n",
    "\n",
    "freq_df.plot(kind=\"bar\", x=\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imported wordcloud function\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#created a word cloud visualize the most common words \n",
    "def wordcloud(counter):\n",
    "    \"\"\"A small wordloud wrapper\"\"\"\n",
    "    wc = WordCloud(width=1200, height=800,\n",
    "                   background_color =\"white\",\n",
    "                   max_words=200)\n",
    "    wc.generate_from_frequencies(counter)\n",
    "\n",
    "    #plot\n",
    "    fig=plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word 2 Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import word2vec\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# tokenize sentences in corpus\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(text) for text in df[\"Text\"]]\n",
    "cleaned_tokenized_corpus = [wpt.tokenize(remove_stopwords(text)) for text in df[\"Text\"]]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 100    # Word vector dimensionality  \n",
    "window_context = 5          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=50)\n",
    "\n",
    "clean_w2v_model = word2vec.Word2Vec(cleaned_tokenized_corpus, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view similar words based on gensim's model\n",
    "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['good', 'bad', 'food', 'market']}\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view similar words based on gensim's model without stopwords \n",
    "clean_similar_words = {search_term: [item[0] for item in clean_w2v_model.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['good', 'bad', 'food', 'market']}\n",
    "clean_similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "wvs = w2v_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "w2v_labels = words\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for w2v_label, x, y in zip(w2v_labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(w2v_label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = sum([[k] + v for k, v in clean_similar_words.items()], [])\n",
    "wvs = clean_w2v_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "w2v_labels = words\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for w2v_label, x, y in zip(w2v_labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(w2v_label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word embeddings using word2vec\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "   \n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    w2v_features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(w2v_features)\n",
    "\n",
    "\n",
    "# get document level embeddings\n",
    "w2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model,\n",
    "                                             num_features=feature_size)\n",
    "pd.DataFrame(w2v_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences in corpus\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(text) for text in df[\"Text\"]]\n",
    "cleaned_tokenized_corpus = [wpt.tokenize(remove_stopwords(text)) for text in df[\"Text\"]]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 100    # Word vector dimensionality  \n",
    "window_context = 5          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=50)\n",
    "\n",
    "clean_w2v_model = word2vec.Word2Vec(cleaned_tokenized_corpus, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PROJECT MILESTONE 2 \n",
    "\n",
    "\n",
    "Tested 4 models Random Forest, Linear SVC, Multinomial NB and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "linearsvc_pred = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels = [\"negative\", \"positive\"], yticklabels=[\"negative\", \"positive\"])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, model, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.3, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "rfc_pred = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels = [\"negative\", \"positive\"], yticklabels=[\"negative\",\"positive\"])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, model, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "NB_pred = y_pred\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels = [\"negative\", \"positive\"], yticklabels=[\"negative\", \"positive\"])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, model, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test,  indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "lgr_pred = y_pred\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels = [\"negative\", \"positive\"], yticklabels=[\"negative\",\"positive\"])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, model, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "lw = 2\n",
    "plt.figure()\n",
    "fpr1, tpr1, _ = metrics.roc_curve(le.fit_transform(y_test),  le.fit_transform(rfc_pred))\n",
    "fpr2, tpr2, _ = metrics.roc_curve(le.fit_transform(y_test),  le.fit_transform(linearsvc_pred))\n",
    "fpr3, tpr3, _ = metrics.roc_curve(le.fit_transform(y_test),  le.fit_transform(NB_pred))\n",
    "fpr4, tpr4, _ = metrics.roc_curve(le.fit_transform(y_test),  le.fit_transform(lgr_pred))\n",
    "auc1 = metrics.roc_auc_score(le.fit_transform(y_test),  le.fit_transform(rfc_pred))\n",
    "auc2 = metrics.roc_auc_score(le.fit_transform(y_test),  le.fit_transform(linearsvc_pred))\n",
    "auc3 = metrics.roc_auc_score(le.fit_transform(y_test),  le.fit_transform(NB_pred))\n",
    "auc4 = metrics.roc_auc_score(le.fit_transform(y_test),  le.fit_transform(lgr_pred))\n",
    "plt.plot(fpr1,tpr1, color='red',lw=lw, label='ROC curve RFC (area = %0.2f)'% auc1)\n",
    "plt.plot(fpr2,tpr2, color='blue',lw=lw, label='ROC curve Linear SVC (area = %0.2f)'% auc2)\n",
    "plt.plot(fpr3,tpr3, color='darkorange',lw=lw, label='ROC curve Naive Bayes (area = %0.2f)'% auc3)\n",
    "plt.plot(fpr4,tpr4, color='green',lw=lw, label='ROC curve Linear Reg (area = %0.2f)'% auc4)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.title('ROC Curve with TFIDF Features')\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X_train, le.fit_transform(y_train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "clf1 = DecisionTreeClassifier(criterion='entropy', max_depth=10)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=1)    \n",
    "\n",
    "bagging1 = BaggingClassifier(base_estimator=clf1, n_estimators=10, max_samples=0.8, max_features=0.8)\n",
    "bagging2 = BaggingClassifier(base_estimator=clf2, n_estimators=10, max_samples=0.8, max_features=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label = ['Decision Tree', 'K-NN', 'Bagging Tree', 'Bagging K-NN']\n",
    "clf_list = [clf1, clf2, bagging1, bagging2]\n",
    "\n",
    "\n",
    "for clf, label in zip(clf_list, label):        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='f1')\n",
    "    accuracy_scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "\n",
    "    print (\"F1 Score: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "    print (\"Acccuracy Score: %.2f (+/- %.2f) [%s]\" %(accuracy_scores.mean(), accuracy_scores.std(), label))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Curve with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, bagging1, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XOR dataset\n",
    "#X = np.random.randn(200, 2)\n",
    "#y = np.array(map(int,np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)))\n",
    "    \n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "\n",
    "num_est = [1, 2, 3, 10]\n",
    "label = ['AdaBoost (n_est=1)', 'AdaBoost (n_est=2)', 'AdaBoost (n_est=3)', 'AdaBoost (n_est=10)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_est, label in zip(num_est, label):     \n",
    "    boosting = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est)   \n",
    "    boosting.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "for learning_rate in lr_list:\n",
    "    boosting = AdaBoostClassifier(base_estimator=clf, n_estimators=10, learning_rate=learning_rate)\n",
    "        \n",
    "    plt.figure()\n",
    "    plot_learning_curves(X_train, y_train, X_test, y_test, boosting, print_model=False, style='ggplot')\n",
    "    plt.show()\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(boosting.score(X_train, y_train)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(boosting.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost With F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_rate in lr_list:\n",
    "#Ensemble Size\n",
    "    num_est = np.linspace(1,20,10).astype(int)\n",
    "    bg_clf_cv_mean = []\n",
    "    bg_clf_cv_std = []\n",
    "    for n_est in num_est:\n",
    "        ada_clf = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est,learning_rate=learning_rate)\n",
    "        scores = cross_val_score(ada_clf, X, y, cv=3, scoring='f1')\n",
    "        bg_clf_cv_mean.append(scores.mean())\n",
    "        bg_clf_cv_std.append(scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_rate in lr_list:\n",
    "    plt.figure()\n",
    "    (_, caps, _) = plt.errorbar(num_est, bg_clf_cv_mean, yerr=bg_clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "    for cap in caps:\n",
    "        cap.set_markeredgewidth(1)                                                                                                                                \n",
    "    plt.ylabel('F1 Score'); plt.xlabel('Ensemble Size'); plt.title('AdaBoost Ensemble learning rate of: '+ str(learning_rate));\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost with Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_rate in lr_list:\n",
    "#Ensemble Size\n",
    "    num_est = np.linspace(1,20,10).astype(int)\n",
    "    acc_bg_clf_cv_mean = []\n",
    "    acc_bg_clf_cv_std = []\n",
    "    for n_est in num_est:\n",
    "        ada_clf = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est,learning_rate=learning_rate)\n",
    "        scores = cross_val_score(ada_clf, X, y, cv=3, scoring='accuracy')\n",
    "        acc_bg_clf_cv_mean.append(scores.mean())\n",
    "        acc_bg_clf_cv_std.append(scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_rate in lr_list:\n",
    "    plt.figure()\n",
    "    (_, caps, _) = plt.errorbar(num_est, acc_bg_clf_cv_mean, yerr=acc_bg_clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "    for cap in caps:\n",
    "        cap.set_markeredgewidth(1)                                                                                                                                \n",
    "    plt.ylabel('Accuracy'); plt.xlabel('Ensemble Size'); plt.title('AdaBoost Ensemble learning rate of: '+ str(learning_rate));\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr1 = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2],meta_classifier=lr1)\n",
    "label = ['KNN', 'Random Forest', \"Naive Bayes\", 'Stacking Classifier']\n",
    "clf_list = [clf1, clf2, clf3, sclf]\n",
    "\n",
    "clf_cv_mean = []\n",
    "clf_cv_std = []\n",
    "acc_clf_cv_mean = []\n",
    "acc_clf_cv_std = []\n",
    "\n",
    "for clf, label in zip(clf_list, label):\n",
    "        \n",
    "    scores = cross_val_score(clf, X, y, cv=2, scoring='f1')\n",
    "    accuracy_scores = cross_val_score(clf, X, y, cv=2, scoring='accuracy')\n",
    "    print (\"F1 Score: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "    print (\"Accuracy: %.2f (+/- %.2f) [%s]\" %(accuracy_scores.mean(), accuracy_scores.std(), label))\n",
    "\n",
    "    clf_cv_mean.append(scores.mean())\n",
    "    clf_cv_std.append(scores.std())\n",
    "    acc_clf_cv_mean.append(accuracy_scores.mean())\n",
    "    acc_clf_cv_std.append(accuracy_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot classifier accuracy    \n",
    "plt.figure()\n",
    "(_, caps, _) = plt.errorbar(range(4), clf_cv_mean, yerr=clf_cv_std, c='blue', fmt='-o', capsize=4)\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)                                                                                                                                \n",
    "plt.xticks(range(4), ['KNN', 'RF', \"Naive Bayes\", 'Stacking Classifier'])        \n",
    "plt.ylabel('f1'); plt.xlabel('Classifier'); plt.title('Stacking Ensemble W/ F1 Score');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot classifier accuracy    \n",
    "plt.figure()\n",
    "(_, caps, _) = plt.errorbar(range(4), acc_clf_cv_mean, yerr=acc_clf_cv_std, c='blue', fmt='-o', capsize=4)\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)                                                                                                                                \n",
    "plt.xticks(range(4), ['KNN', 'RF', \"Naive Bayes\", 'Stacking Classifier'])        \n",
    "plt.ylabel('accurancy'); plt.xlabel('Classifier'); plt.title('Stacking Ensemble W/ Accuracy Score');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, le.fit_transform(y_train), X_test, le.fit_transform(y_test), sclf, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVC with Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVCmodel = LinearSVC()\n",
    "X = w2v_feature_array\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle = True, stratify = None)\n",
    "SVCmodel.fit(X_train, y_train)\n",
    "y_pred = SVCmodel.predict(X_test)\n",
    "linearsvc_pred = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels = [\"negative\", \"positive\"], yticklabels=[\"negative\",\"positive\"])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, SVCmodel, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RfCmodel = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle = True, stratify = None)\n",
    "RfCmodel.fit(X_train, y_train)\n",
    "y_pred = RfCmodel.predict(X_test)\n",
    "rfc_pred = y_pred\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels = [\"negative\",\"positive\"], yticklabels=[\"negative\",\"positive\"])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, RfCmodel, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Gaussian NB to take into account negative values \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "GNBmodel = GaussianNB()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle = True, stratify = None)\n",
    "GNBmodel.fit(X_train, y_train)\n",
    "y_pred = GNBmodel.predict(X_test)\n",
    "gnb_pred = y_pred\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels = [\"negative\",\"positive\"], yticklabels=[\"negative\",\"positive\"])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, GNBmodel, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test,  indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "lgr_pred = y_pred\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels = [\"negative\", \"positive\"], yticklabels=[\"negative\",\"positive\"])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, model, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fpr1, tpr1, _ = metrics.roc_curve(le.fit_transform(y_test),  le.fit_transform(rfc_pred))\n",
    "fpr2, tpr2, _ = metrics.roc_curve(le.fit_transform(y_test),  le.fit_transform(linearsvc_pred))\n",
    "fpr3, tpr3, _ = metrics.roc_curve(le.fit_transform(y_test),  le.fit_transform(NB_pred))\n",
    "fpr4, tpr4, _ = metrics.roc_curve(le.fit_transform(y_test),  le.fit_transform(lgr_pred))\n",
    "auc1= metrics.roc_auc_score(le.fit_transform(y_test),  le.fit_transform(rfc_pred))\n",
    "auc2 = metrics.roc_auc_score(le.fit_transform(y_test),  le.fit_transform(linearsvc_pred))\n",
    "auc3 = metrics.roc_auc_score(le.fit_transform(y_test),  le.fit_transform(NB_pred))\n",
    "auc4 = metrics.roc_auc_score(le.fit_transform(y_test),  le.fit_transform(lgr_pred))\n",
    "plt.plot(fpr1,tpr1, color='red',lw=lw, label='ROC curve RFC (area = %0.2f)'% auc1)\n",
    "plt.plot(fpr2,tpr2, color='blue',lw=lw, label='ROC curve Linear SVC (area = %0.2f)'% auc2)\n",
    "plt.plot(fpr3,tpr3, color='darkorange',lw=lw, label='ROC curve Naive Bayes (area = %0.2f)'% auc3)\n",
    "plt.plot(fpr4,tpr4, color='green',lw=lw, label='ROC curve Linear Reg (area = %0.2f)'% auc4)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.title('ROC Curve with Word2Vec Features')\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Models using Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X_train, le.fit_transform(y_train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#XOR dataset\n",
    "#X = np.random.randn(200, 2)\n",
    "#y = np.array(map(int,np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)))\n",
    "    \n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['Decision Tree', 'K-NN', 'Bagging Tree', 'Bagging K-NN']\n",
    "clf_list = [clf1, clf2, bagging1, bagging2]\n",
    "\n",
    "\n",
    "for clf, label in zip(clf_list, label):        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='f1')\n",
    "    accuracy_scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "\n",
    "    print (\"F1 Score: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "    print (\"Acccuracy Score: %.2f (+/- %.2f) [%s]\" %(accuracy_scores.mean(), accuracy_scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, bagging1, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOOSTING WITH WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=10)\n",
    "\n",
    "num_est = [1, 2, 3, 10]\n",
    "label = ['AdaBoost (n_est=1)', 'AdaBoost (n_est=2)', 'AdaBoost (n_est=3)', 'AdaBoost (n_est=10)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_est, label in zip(num_est, label):     \n",
    "    boosting = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est)   \n",
    "    boosting.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "for learning_rate in lr_list:\n",
    "    boosting = AdaBoostClassifier(base_estimator=clf, n_estimators=10, learning_rate=learning_rate)\n",
    "        \n",
    "    plt.figure()\n",
    "    plot_learning_curves(X_train, y_train, X_test, y_test, boosting, print_model=False, style='ggplot')\n",
    "    plt.show()\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(boosting.score(X_train, y_train)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(boosting.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_rate in lr_list:\n",
    "#Ensemble Size\n",
    "    num_est = np.linspace(1,20,10).astype(int)\n",
    "    bg_clf_cv_mean = []\n",
    "    bg_clf_cv_std = []\n",
    "    for n_est in num_est:\n",
    "        ada_clf = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est,learning_rate=learning_rate)\n",
    "        scores = cross_val_score(ada_clf, X, y, cv=3, scoring='f1')\n",
    "        bg_clf_cv_mean.append(scores.mean())\n",
    "        bg_clf_cv_std.append(scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_rate in lr_list:\n",
    "    plt.figure()\n",
    "    (_, caps, _) = plt.errorbar(num_est, bg_clf_cv_mean, yerr=bg_clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "    for cap in caps:\n",
    "        cap.set_markeredgewidth(1)                                                                                                                                \n",
    "    plt.ylabel('f1'); plt.xlabel('Ensemble Size'); plt.title('AdaBoost Ensemble learning rate of: '+ str(learning_rate));\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr1 = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2],meta_classifier=lr1)\n",
    "label = ['KNN', 'Random Forest', \"Naive Bayes\", 'Stacking Classifier']\n",
    "clf_list = [clf1, clf2, clf3, sclf]\n",
    "\n",
    "clf_cv_mean = []\n",
    "clf_cv_std = []\n",
    "acc_clf_cv_mean = []\n",
    "acc_clf_cv_std = []\n",
    "\n",
    "for clf, label in zip(clf_list, label):\n",
    "        \n",
    "    acc_scores = cross_val_score(clf, X, y, cv=2, scoring='accuracy')\n",
    "    print (\"Accuracy: %.2f (+/- %.2f) [%s]\" %(acc_scores.mean(), acc_scores.std(), label))\n",
    "    scores = cross_val_score(clf, X, y, cv=2, scoring='f1')\n",
    "    print (\"F1 Score: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "    clf_cv_mean.append(scores.mean())\n",
    "    clf_cv_std.append(scores.std())\n",
    "    acc_clf_cv_mean.append(scores.mean())\n",
    "    acc_clf_cv_std.append(scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot classifier f1 score    \n",
    "plt.figure()\n",
    "(_, caps, _) = plt.errorbar(range(4), clf_cv_mean, yerr=clf_cv_std, c='blue', fmt='-o', capsize=4)\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)                                                                                                                                \n",
    "plt.xticks(range(4), ['KNN', 'RF', \"Naive Bayes\", 'Stacking Classifier'])        \n",
    "plt.ylabel('F1'); plt.xlabel('Classifier'); plt.title('Stacking Ensemble');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, le.fit_transform(y_train), X_test, le.fit_transform(y_test), sclf, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting and Explaining Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification to the lime intepreter does not work with a linearsvc so we used lightning boost method which returns a probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "new_labels = le.fit_transform(labels)\n",
    "y = new_labels\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, df.index, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'goss',\n",
    "    'objective': 'binary',\n",
    "    'metric':'binary_logloss',\n",
    "    'metric': {'l2', 'auc'},\n",
    "    'num_leaves': 50,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'verbose': None,\n",
    "    'num_iteration':100,\n",
    "    'num_threads':7,\n",
    "    'max_depth':12,\n",
    "    'min_data_in_leaf':100,\n",
    "    'alpha':0.5}\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test)\n",
    "\n",
    "\n",
    "# training the lightgbm model\n",
    "model = lgb.train(lgb_params,lgb_train,num_boost_round=20,valid_sets=lgb_eval,early_stopping_rounds=5)\n",
    "\n",
    "\n",
    "\n",
    "# this is required as LIME requires class probabilities in case of classification example\n",
    "# LightGBM directly returns probability for class 1 by default \n",
    "\n",
    "def prob(data):\n",
    "    return np.array(list(zip(1-model.predict(data),model.predict(data))))\n",
    "    \n",
    "\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(training_data = features, \n",
    "                                                   mode = 'classification',\n",
    "                                                   training_labels = new_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will take a row number and explain the prediction by giving the probability and give which\n",
    "#which features impacted the result \n",
    "def model_explained(i):\n",
    "    exp = explainer.explain_instance(features[i],predict_fn = prob)\n",
    "\n",
    "# Plot the prediction explaination\n",
    "    exp.as_pyplot_figure();\n",
    "    exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#line 456\n",
    "#we see here that the sentiment is more positive then negative \n",
    "model_explained(456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_explained(678)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Interpretation with different models applied in project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_interpret(model, vec, row_number):\n",
    "    model = str(model)\n",
    "    model = model.lower()\n",
    "    vec = str(vec)\n",
    "    vec = vec.lower()\n",
    "    row_number = int(row_number)\n",
    "    if model == \"multinb\" and vec ==\"word2vec\":\n",
    "        return print(\"Multinomial does not take negative values please pick other model or choose tfidf Word embeddings\")\n",
    "    \n",
    "    if vec == \"tfdif\":\n",
    "        X = features\n",
    "        new_labels = le.fit_transform(labels)\n",
    "        y = new_labels\n",
    "        X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, df.index, test_size=0.3, random_state=42)\n",
    "    elif vec == \"word2vec\":\n",
    "        X = w2v_feature_array\n",
    "        new_labels = le.fit_transform(labels)\n",
    "        y = new_labels\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle = True, stratify = None)\n",
    "    else:\n",
    "        return print(\"Please enter Tfidf or Word2Vec\")\n",
    "    \n",
    "    if model == \"randomf\":\n",
    "        model = RandomForestClassifier()\n",
    "    elif model == \"gaussiannb\":\n",
    "        model = GaussianNB()\n",
    "    elif model == \"multinb\":\n",
    "        model = MultinomialNB()\n",
    "    elif model == \"logreg\":\n",
    "        model = LogisticRegression()\n",
    "    elif model == \"linearsvc\":\n",
    "        model = LinearSVC()\n",
    "    else: \n",
    "        return print(\"Please enter RandomF, GaussianNB, MultiNB, LogReg or LinearSVC\")\n",
    "\n",
    "        \n",
    "    # Fit and test on the features\n",
    "    model.fit(X, y)\n",
    "    model_pred = model.predict(X_test)\n",
    "    number_of_pred = len(model_pred)\n",
    "    \n",
    "    if row_number < 0:\n",
    "        return print(\"Please enter number bigger or equal to 0\")\n",
    "    elif row_number > number_of_pred:\n",
    "        return print(\"Please enter number smaller or equal to \" + str(number_of_pred))\n",
    "    else: \n",
    "        row_number = row_number\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create a lime explainer object\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(training_data = X, \n",
    "                                                   mode = 'regression',\n",
    "                                                   training_labels = new_labels)                                                  \n",
    "\n",
    "\n",
    "    # Explanation for wrong prediction\n",
    "    #exp = explainer.explain_instance(data_row = wrong, predict_fn = model.predict)\n",
    "    exp = explainer.explain_instance(X[row_number],predict_fn = model.predict)\n",
    "\n",
    "    # Plot the prediction explaination\n",
    "    return exp.show_in_notebook()        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_interpret(model='GaussianNB', vec='tfdif', row_number=258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_interpret(model='randomf', vec='tfdif', row_number=258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_interpret(model='logreg', vec='tfdif', row_number=258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_interpret(model='multinb', vec='tfdif', row_number=258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_interpret(model='linearsvc', vec='tfdif', row_number=258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_interpret(model='linearsvc', vec='word2vec', row_number=258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_interpret(model='logreg', vec='word2vec', row_number=258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_interpret(model='randomf', vec='word2vec', row_number=258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_interpret(model='GaussianNB', vec='word2vec', row_number=258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_interpret(model='multinb', vec='word2vec', row_number=258)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification Using LSTM W/ Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "# Others\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(features, maxlen=50)\n",
    "\n",
    "## Network architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 100, input_length=50))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit the model\n",
    "model.fit(data, le.fit_transform((labels)), validation_split=0.4, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embds = model.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for word, i in counter.items():\n",
    "    word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2).fit_transform(word_embds)\n",
    "number_of_words = 1000\n",
    "trace = go.Scatter(\n",
    "    x = X_embedded[0:number_of_words,0], \n",
    "    y = X_embedded[0:number_of_words, 1],\n",
    "    mode = 'markers',\n",
    "    text= word_list[0:number_of_words]\n",
    ")\n",
    "layout = dict(title= 't-SNE 1 vs t-SNE 2 for first 1000 words ',\n",
    "              yaxis = dict(title='t-SNE 2'),\n",
    "              xaxis = dict(title='t-SNE 1'),\n",
    "              hovermode= 'closest')\n",
    "fig = dict(data = [trace], layout= layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented K Means Cluster with the Tdif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(9, 10))\n",
    "X = vectorizer.fit_transform(df[\"Text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=300, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :20]:\n",
    "        print('%s' % terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
